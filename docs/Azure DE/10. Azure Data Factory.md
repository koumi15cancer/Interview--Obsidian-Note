### Summary of the Video: Introduction to Azure Data Factory


![[Introduction to Azure Data Factory.png]]
#### Introduction
The video introduces Azure Data Factory (ADF) as a service for loading and orchestrating data in a data lake. The focus is on using ADF to ingest data from various sources into a data lake, specifically the raw (row) layer, which stores immutable copies of source data.

#### Key Concepts
1. **Basic BI Flow**:
   - **Data Sources**: Various types like CSV, Excel, JSON, SQL databases, and APIs.
   - **Row Layer**: The first layer in a data lake where raw, unprocessed data is stored.
   - **Data Lake Storage**: Using Azure Data Lake Storage Gen2 (ADLS Gen2) for storing data.
   - **Transformation Layer**: Where data is cleaned and business logic is applied.
   - **Data Warehouse and Power BI**: For storing processed data and creating reports.

2. **Azure Data Factory (ADF)**:
   - **Data Ingestion**: ADF can copy data from various sources to the row layer in ADLS Gen2.
   - **Orchestration**: ADF orchestrates data workflows, executing tasks in a specific order.

#### Setting Up ADF
1. **Linked Services**:
   - Connections to data sources and destinations.
   - Example: Linked service for Azure SQL Database and another for ADLS Gen2.

2. **Data Sets**:
   - Representations of data associated with linked services.
   - Example: Data set for a customer table in Azure SQL Database and another for CSV files in ADLS Gen2.

3. **Pipelines**:
   - Logical grouping of activities that define the workflow.
   - Example: A simple pipeline to copy data from Azure SQL Database to ADLS Gen2.

4. **Activities**:
   - Tasks within a pipeline, such as the "Copy Data" activity to move data from one place to another.

5. **Triggers**:
   - Define when and how often pipelines should run.
   - Example: Schedule trigger to run the pipeline at a specific time.

#### Example Scenario
1. **Setup**:
   - Create an Azure SQL Database and a storage account (ADLS Gen2).
   - Organize data lake directories to store data systematically.

2. **Using ADF**:
   - Create linked services for the SQL Database and ADLS Gen2.
   - Create data sets for the customer table in SQL Database and for CSV files in ADLS Gen2.
   - Create a pipeline with a "Copy Data" activity to move data from SQL Database to ADLS Gen2.

3. **Execution**:
   - Use ADF to run the pipeline and verify that data is correctly copied to the data lake.

#### Conclusion
Azure Data Factory simplifies data ingestion and orchestration, making it a powerful tool for loading and transforming data in a data lake. The video demonstrates a basic scenario, highlighting ADF's capabilities and ease of use. Future episodes will cover more advanced features and best practices for securing and optimizing ADF pipelines.
