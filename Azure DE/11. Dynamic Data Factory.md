### Summary of the Video: Dynamic Pipelines in Azure Data Factory


![[Dynamic Pipelines in Azure Data Factory.png]]
#### Introduction
The video covers the process of creating dynamic pipelines in Azure Data Factory (ADF) to handle multiple tables from a database, enhancing the flexibility and scalability of data ingestion workflows.

#### Key Concepts
1. **Dynamic Pipelines**:
   - Moving from static to dynamic design to handle multiple tables.
   - Using ADF's features to make pipelines flexible enough to process varying numbers of tables.

2. **Linked Services**:
   - Dynamic Linked Services for SQL databases, allowing runtime specification of server and database names.

3. **Data Sets**:
   - Creating generic data sets that can represent data from any table.
   - Parameterizing data sets to make them dynamic and reusable.

4. **Pipeline Activities**:
   - **Lookup Activity**: Retrieves a list of all tables from a SQL database.
   - **For Each Activity**: Iterates over the list of tables.
   - **Copy Activity**: Copies data from each table to a target location in the data lake.

#### Implementation Steps
1. **Setup**:
   - Connect to the Azure SQL database using SQL Server Management Studio (SSMS).
   - List multiple tables in the database for processing.

2. **Design**:
   - Create a dynamic linked service for Azure SQL Database.
   - Define generic data sets for both source (SQL database) and target (Data Lake Storage).

3. **Pipeline Creation**:
   - Create a new pipeline and add a Lookup activity to get a list of tables.
   - Add a For Each activity to iterate over the list of tables.
   - Within the For Each activity, add a Copy activity to transfer data from each table to the data lake.

4. **Parameterization**:
   - Add parameters to the linked service (server name and database name).
   - Use these parameters in the data set to dynamically connect to the database.
   - Pass parameters from the pipeline to the data set and linked service.

5. **Dynamic Querying**:
   - Construct dynamic SQL queries to select data from each table based on schema and table name.

6. **Target Configuration**:
   - Define the structure for saving files in the data lake dynamically.
   - Create parameters for directory paths and file names in the target data set.

#### Execution and Verification
1. **Run the Pipeline**:
   - Execute the pipeline to process multiple tables.
   - Check the data lake to verify that data from each table is saved in the correct format and location.

2. **Output Handling**:
   - Inspect the output to ensure that each table's data is correctly ingested and saved.

#### Conclusion
Azure Data Factory's dynamic pipelines provide a powerful way to handle flexible and scalable data ingestion workflows. By leveraging dynamic linked services, data sets, and pipeline activities, users can create robust solutions to process varying numbers of tables and datasets efficiently.
